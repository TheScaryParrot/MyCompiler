\chapter{Funktionsweise traditioneller Compiler} \label{cha:3-Tradional_Compiler}

Der traditionelle Aufbau eines Compilers lässt sich mit folgendem Schema veranschaulichen:

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.2]{resources/images/mountain.png}
    \caption[Schritte, die ein Compiler durchläuft (https://github.com/munificent/craftinginterpreters, besucht am 5.8.2024)]{Schritte, die ein Compiler durchläuft}
    \label{fig:mountain}
\end{figure}

In dieser Arbeit werde ich mich auf die in der unteren Abbildung \ref{fig:mountain-edited} dargestellten Schritte fokussieren.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.2]{resources/images/mountain-edited.png}
    \caption[Schritte, die in dieser Arbeit behandelt werden (Basierend auf Abbildung \ref{fig:mountain})]{Schritte, die in dieser Arbeit behandelt werden}
    \label{fig:mountain-edited}
\end{figure}

Die von mir in dies Maturaarbeit verwendeten Fachbegriffe entsprechen nicht immer denen aus Abbildung \ref{fig:mountain-edited}.
Bei der ersten Verwendung dieser Fachbegriffe wird jeweils die alternative aus Abbildung \ref{fig:mountain-edited} in Klammern angegeben.

Als Basis für diese Kapitel dienen die Quellen \cite{CompilersDragon} und \cite{Lecture}.

\section{Lexikalische Analyse (\textit{Scanning})}
Meist werden Programme so geschrieben, dass wir Menschen sie lesen und verstehen können. Dafür verwendet man Buchstaben, Zahlen, Sonderzeichen (wie + oder *) und Whitespaces (wie Leerzeichen oder Absätze).
Diese Zeichen sind jedoch für den Computer nicht sofort verständlich. Der erste Schritt beim Kompilieren ist daher die \textit{lexikalische Analyse}. Diese Analyse wird von einem Teil des Compilers, dem \textit{Lexer}, durchgeführt.
Die Aufgabe dieses Lexers ist es, die Eingabedatei zu analysieren und die gefundenen Zeichen in sogenannte \textit{Tokens} zu verwandeln. Diese Tokens sind Datenstrukturen, die der Compiler versteht und mit denen er weiterarbeiten kann.

Ein Beispiel der lexikalischen Analyse auf der Programmiersprache C:

\begin{lstlisting}[language=C, label=eg:preLex, caption=C code vor lexikalischer Analyse]
int foo()
{
    if (bar == 0)
    {
        return 0;
    }

    return 1;
}
\end{lstlisting}

\begin{lstlisting}[label=eg:postLex, caption=Tokens nach lexikalischer Analyse]
Keyword         (keyword="int")
Identifier      (id="foo")
LParenthesis
RParenthesis
Keyword         (keyword="if")
LParenthesis
Identifier      (id="bar")
Operator        (operator=ComparisonEqual)
LiteralInt      (value=0)
[...]
\end{lstlisting}

Der Lexer legt fest, welche Zeichen die Eingabe-Programmiersprache enthalten darf und welche Bedeutung ihnen zugesprochen wird. So ist zum Beispiel im Lexer festgelegt, dass ein + Zeichen als Addition interpretiert wird.
Genauso wie im Listing \ref{eg:postLex} {\listingFont\selectfont 'if'} als Keyword-Token gesehen wird, lässt sich im Lexer auch bestimmen, dass ein Wort wie {\listingFont\selectfont 'else'} als Keyword angesehen werden soll.

\section{Syntaktische Analyse (\textit{Parsing})}
Der Compiler hat nun die Zeichen der Eingabedatei in ein für ihn verständliches Format übersetzt.
Jedoch fehlt dem Compiler noch das Verständnis für die Syntax der Eingabe-Programmiersprache.
Die meisten High-Level Programmiersprachen weisen Syntaxregeln auf. Diese beinhalten, wie Funktionen und Variablen definiert werden oder mit welchen Punktvorstrich-Regeln Expressions evaluiert werden.
In diesem Schritt führt der sogenannte \textit{Parser} die \textit{syntaktische Analyse} durch.
Dabei werden die bei der lexikalischen Analyse gefundenen Tokens ineinander verschachtelt und in einen sogenannten \textit{Abstract Syntax Tree (AST)} überführt.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{resources/images/AST.pdf}
    \caption[AST zum euklidischen Algorithmus. (https://en.wikipedia.org/wiki/Abstract\_syntax\_tree, besucht am 5.8.2024)]{AST zum euklidischen Algorithmus}
    \label{fig:syntax-tree}
\end{figure}

Ein AST enthält somit nicht nur Informationen über die Tokens, sondern über die gesamten Strukturen und Abhängigkeiten, die sich aus den Tokens ergeben. Variabel- und Funktionsdefinitionen oder komplexe Statements wie 'if' oder 'for'
sind im AST als \textit{Nodes} enthalten. Wenn man die Nodes des AST von unten nach oben durchquert, erhält man die Reihenfolge der einzelnen Tokens ohne Abhängigkeitskonflikte.
Eine Subtraktion kann zum Beispiel erst ausgeführt werden, wenn sowohl die linke als auch die rechte Zahl bekannt ist.
Daher befindet sich, wie in Abbildung \ref{fig:syntax-tree} ersichtlich, die Subtraktion über den beiden benötigten Werten im AST.

\section{Semantische Analyse (\textit{Analysis})}
Semantik ist die Wissenschaft der Bedeutung von Wörtern einer Menschensprache. Bei einem Compiler geht es bei der \textit{semantische Analyse} nicht um Bedeutung sondern um die Korrektheit von Expressions.
Wird eine Variable nicht konform ihres Datentyps verwendet, zum Beispiel wenn zwei Strings dividiert werden sollen, wird dies während der semantischen Analyse entdeckt und gemeldet.
Gegebenenfalls kann ein impliziter Cast, also ein impliziter Wechsel des Datentyps, hinzugefügt werden.
So geben zum Beispiel manche Programmiersprachen bei der Division zweier Ganzzahlen eine Fliesskommazahl zurück.
Auch werden unbekannte Variablen und Funktionen in diesem Schritt abgefangen.
Weiter wird der Datentyp einer Node des AST an diese angebunden. Nach der semantischen Analyse sieht der AST aus Abbildung \ref{fig:syntax-tree} wie folgt aus:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{resources/images/AST_with_types.pdf}
    \caption[AST nach der semantischen Analyse (Basierend auf Abbildung \ref{fig:syntax-tree})]{AST nach der semantischen Analyse}
    \label{fig:syntax-tree-with-types}
\end{figure}

Der AST wird nach der semantischen Analyse meist als Zwischencode (\textit{Intermediate Representation}) bezeichnet.
Da dieser Zwischencode in den weiteren Beispielen jedoch weiterhin dem Aufbau eines AST folgt, werde ich auch weiter AST als Begriff verwenden.


\section{Codegenerierung} \label{sec:traditional_code_generation}
\textit{Codegenerierung} ist der finale und oft auch komplexeste Schritt, der ein Compiler ausführen muss.
Nun da die Eingabedatei als AST vorliegt, kann die Ausgabedatei generiert werden. Eine geläufige Methode der Codegenerierung ist die sogenannte \textit{Macro Expansion}.
Dabei wird der AST von unten nach oben schrittweise mit Teilen an Ausgabecode ersetzt.
Diese Ausgabecode-Teile sind häufig von den darunterliegenden Nodes abhängig. Der AST aus Abbildung \ref{fig:syntax-tree-with-types} sähe nach erfolgreicher \textit{Macro Expansion} wie folgt aus:

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{resources/images/AST_macro_expansion.pdf}
    \caption[AST nach \textit{Macro Expansion}. (Basierend auf Abbildung \ref{fig:syntax-tree})]{AST nach \textit{Macro Expansion}}
    \label{fig:syntax-tree-after-macro-expansion}
\end{figure}

\section{Optimierung}
Codegenerierung ist zwar der letzte Schritt beim Kompilieren, trotzdem wurde eine wichtige Aufgabe des Compilers noch nicht betrachtet. \textit{Optimierung} geschieht zwischen jedem der genannten Schritte und dies häufig mehrmals.
Dabei geht es darum die Ausgabedatei so effizient wie möglich zu machen. Effizient kann hierbei verschiedenes bedeuten.
Die Ausgabedatei muss so schnell wie möglich ausgeführt werden, Memory sparsam verwenden und dazu noch eine möglichst kleine Datei umfassen. 
Optimierungsmethoden reichen vom Überspringen der Kommentare und Umstellen von mathematischen Operationen, bis zum Entfernen von ungebrauchten Variablen und sogenannten Deadstores.
Es muss von CPU-Registern profitiert, mit Heap-Memory umgegangen und von Inline-Funktionen Gebrauch gemacht werden. Optimierung ist also sehr vielseitig und komplex.
Wie Optimierung genau aussehen kann, wird daher in dieser Maturaarbeit nicht weiter betrachtet.
