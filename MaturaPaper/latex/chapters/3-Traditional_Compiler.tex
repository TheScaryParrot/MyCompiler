\chapter{Funktionsweise traditioneller Compiler} \label{cha:3-Tradional_Compiler}

Der traditionelle Aufbau eines Compilers lässt sich mit folgendem Schema veranschaulichen:

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.2]{resources/images/mountain.png}
    \caption[Schritte, die ein Compiler durchläuft (https://github.com/munificent/craftinginterpreters, besucht am 5.8.2024)]{Schritte, die ein Compiler durchläuft}
    \label{fig:mountain}
\end{figure}

In dieser Arbeit werde ich mich auf die in der unteren Abbildung \ref{fig:mountain-edited} dargestellten Schritte fokussieren.

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.2]{resources/images/mountain-edited.png}
    \caption[Schritte, die in dieser Arbeit behandelt werden (Basierend auf Abbildung \ref{fig:mountain})]{Schritte, die in dieser Arbeit behandelt werden}
    \label{fig:mountain-edited}
\end{figure}

Die von mit verwendeten Fachbegriffe entsprechen hierbei nicht immer denen aus Abbildung \ref{fig:mountain-edited}.

Als Basis für diese Kapitel dient grösstenteils eine Vorlesung der Universität Bern, die ich für freundlicherweise besuchen durfte.

\section{Lexical Analysis}
Meist werden Programme so geschrieben, dass wir Menschen sie lesen und verstehen können. Dafür verwendet man Buchstaben, Zahlen, Sonderzeichen (wie + oder *) und Whitespaces (wie Leerzeichen oder Absätze).
Diese Zeichen sind jedoch für den Computer nicht sofort verständlich. Der erste Schritt beim Kompilieren ist daher die \textit{Lexical Analysis}. Diese Analyse wird von einem Teil des Compilers, dem \textit{Lexer}, durchgeführt.
Die Aufgabe dieses Lexers ist es, die Eingabedatei zu analysieren und die gefundenen Zeichen in sogenannte \textit{Tokens} zu verwandeln. Diese Tokens sind Datenstrukturen, die der Compiler versteht und mit denen er weiterarbeiten kann.

Ein Beispiel der Lexical Analysis auf der Programmiersprache C:

\begin{lstlisting}[language=C, label=eg:preLex, caption=C code vor Lexical Analysis]
int foo()
{
    if (bar == 0)
    {
        return 0;
    }

    return 1;
}
\end{lstlisting}

\begin{lstlisting}[label=eg:postLex, caption=Tokens nach Lexical Analysis]
Keyword         (keyword="int")
Identifier      (id="foo")
LParenthesis
RParenthesis
Keyword         (keyword="if")
LParenthesis
Identifier      (id="bar")
Operator        (operator=ComparisonEqual)
LiteralInt      (value=0)
[...]
\end{lstlisting}

Der Lexer legt fest, welche Zeichen die Eingabe-Programmiersprache enthalten darf und welche Bedeutung ihnen zugesprochen wird. So ist zum Beispiel im Lexer festgelegt, dass ein + Zeichen als Addition interpretiert wird.
Genauso wie im Listing \ref{eg:postLex} {\listingFont\selectfont 'if'} als Keyword-Token gesehen wird, lässt sich im Lexer auch bestimmen, dass ein Wort wie {\listingFont\selectfont 'else'} als Keyword angesehen werden soll.

\section{Syntax Analysis}
Der Compiler hat nun die Zeichen der Eingabedatei in ein für ihn verständliches Format übersetzt.
Jedoch fehlt dem Compiler noch das Verständnis für die Syntax der Eingabe-Programmiersprache.
Die meisten High-Level Programmiersprachen weisen Syntaxregeln auf. Diese beinhalten, wie Funktionen und Variablen definiert werden oder mit welchen Punktvorstrich-Regeln Expressions evaluiert werden.
In diesem Schritt führt der sogenannte \textit{Parser} die \textit{Syntax Analysis} durch.
Dabei werden die bei der Lexical Analysis gefundenen Tokens ineinander verschachtelt und in einen sogenannten \textit{Abstract Syntax Tree (AST)} überführt.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{resources/images/syntaxtree.svg.png}
    \caption[Abstract Syntax Tree (https://en.wikipedia.org/wiki/Abstract\_syntax\_tree, besucht am 5.8.2024)]{Abstract Syntax Tree zum Euklidischen Algorithmus}
    \label{fig:syntax-tree}
\end{figure}

Ein AST enthält somit nicht nur Informationen über die Tokens, sondern über die gesamten Strukturen und Abhängigkeiten, die sich aus den Tokens ergeben. Variabel- und Funktionsdefinitionen oder komplexe Statements wie 'if' oder 'for'
sind im AST als \textit{Nodes} enthalten. Wenn man die Nodes des AST von unten nach oben durchquert, erhält man die Reihenfolge der einzelnen Tokens ohne Abhängigkeitskonflikte.
Eine Subtraktion kann zum Beispiel erst ausgeführt werden, wenn sowohl die linke als auch die rechte Zahl bekannt ist.
Daher befindet sich, wie in Abbildung \ref{fig:syntax-tree} ersichtlich, die Subtraktion über den beiden benötigten Werten im AST.

\section{Semantic Analysis}
Semantik ist die Wissenschaft der Bedeutung von Wörtern einer Menschensprache. Bei einem Compiler geht es bei der \textit{Semantic Analysis} nicht um Bedeutung sondern um die Korrektheit von Expressions.
Wird eine Variable nicht konform ihres Datentyps verwendet, zum Beispiel wenn zwei Strings dividiert werden sollen, wird dies während der Semantic Analysis entdeckt und gemeldet.
Gegebenenfalls kann ein impliziter Cast, also ein impliziter Wechsel des Datentyps, hinzugefügt werden.
So geben zum Beispiel manche Programmiersprachen bei der Division zweier Integers eine Float zurück.
Auch werden unbekannte Variablen und Funktionen in diesem Schritt abgefangen.
Weiter wird der Datentyp einer Node des AST an diese angebunden.

\section{Code Generation} \label{sec:traditional_code_generation}
\textit{Code Generation} ist der finale und oft auch komplexeste Schritt, der ein Compiler ausführen muss.
Nun da die Eingabedatei als AST vorliegt, kann die Ausgabedatei generiert werden. Eine geläufige Methode der Code Generation ist die sogenannte \textit{Macro Expansion}.
Hierbei wird der AST von unten nach oben schrittweise mit Teilen an Ausgabecode ersetzt.
Diese Ausgabecode-Teile sind häufig von den darunterliegenden Nodes abhängig. 

MAYBE FIGURE OR LISTING AS EXAMPLE

\section{Optimization}
Code Generation ist zwar der letzte Schritt beim Kompilieren, trotzdem wurde eine wichtige Aufgabe des Compilers noch nicht betrachtet. \textit{Optimization} geschieht zwischen jedem der genannten Schritte und dies häufig mehrmals.
Dabei geht es darum die Ausgabedatei so effizient wie möglich zu machen. Effizient kann hierbei verschiedenes bedeuten.
Die Ausgabedatei muss so schnell wie möglich ausgeführt werden, Memory sparsam verwenden und dazu noch eine möglichst kleine Datei umfassen. 
Optimization reicht vom Überspringen der Kommentare und Umstellen von mathematischen Operationen, bis zum Entfernen von ungebrauchten Variablen und sogenannten Deadstores.
Es muss von CPU-Registern profitiert, mit Heap-Memory umgegangen und von Inline-Funktionen Gebrauch gemacht werden. Compiler Optimization ist also sehr vielseitig und komplex.
Wie Optimization genau aussehen kann, wird daher in dieser Maturaarbeit nicht weiter betrachtet.